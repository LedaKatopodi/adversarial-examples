# adversarial-examples

The purpose of this project is dual: primarily, to build a CNN model to accurately classify input images from the MNIST and CIFAR10 datasets; as an extension, to create adversarial examples that can trick the CNN into mis-classifying the input.

Adversarial examples are inputs to machine learning models that intentionally cause the model to make a mistake; in that sense they're like optical illusions for machines.

![Illusion](/aes/illusion.jpeg)

## ðŸ“— Introduction

## ðŸ”‘ Prerequisites

## ðŸ‘Ÿ Walkthrough
